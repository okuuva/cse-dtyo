% State of the Art Chapter

\section{Software Composition Analysis}

As the third party software and OSS specifically has become a critical part of practically every
bigger software project, commercial and proprietary products included \cite{blah}, it has become
increasingly difficult to keep track of what third party components are included in a given
project. This is quite problematic, as the OSS components included can contain security flaws that
can cause serious harm for the companies and their customers in the form of data loss, leaking
intellectual property and so forth. Luckily there are automatic tools available that can keep track
of third party components included in the project and notify the developers when new
vulnerabitilies are reported in the used components and when there's updates available
\cite{pittenger2016know}.

Software composition analysis has steadily raised its profile over the years \cite{blah}. While it
still hasn't quite reached a mainstream status yet, it has been made available for the masses by
actors such as Docker with the introduction of Docker Security Scan in Docker Hub and Docker Cloud
hosting services \cite{dockerscan}.

There are multiple commercial SCA solutions available from providers such as Black Duck Software,
\cite{blackduckhub}, Veracode \cite{veracodesca}, WhiteHat security \cite{whitehatsentinel},
Whitesource \cite{whitesourceosi} and Synopsys \cite{synopsysprotecode}. The main purpose of these
products is to report license information and known vulnerabilities of the identified third party
libraries from the system under test. There are multiple different ways to identify the third party
components. One common way is to statically analyse build files from the source code repository and
deduce what third party libraries are included when the project is compiled. Another way is to try
to figure out what third party library the analyzed code belongs to.

\subsection{Static Binary Analysis}

Static binary analysis is conducted by analyzing the compiled binary without actually executing it.
Working with the binary has some upsides compared to source code: by analyzing the binary we are
analyzing the very same thing the end users are using, thus eliminating the possibility of some
false positives which could result in source code analysis when some or all third party component
dependencies are included using dynamic links by the compiler. There is also the obvious benefit of
not needing to have an access to the source code, which is usually hard or impossible with
proprietary software. However, like Song et al. point out \cite{song2008bitblaze}, there are some
serious down sides to it as well, like additional complexity compared to source code analysis and
making the analysis architecture independent, i.e. supporting binaries compiled for different
processor architectures such as ARM, x86 and MIPS.

There are several different methods that can be utilised when conducting SBA. One quite common
approach is using some intermediate language by first running the binary through a disassembler,
then changing the instructions to the IL and finally running the analysis on the resulting pseudo
assembly code \cite{song2008bitblaze, brumley2011bap}. This approach has the advantage of being
more or less architecture independent, given that the tool that converts assembly instructions to
the IL can handle the varying assembly languages of different processor architectures. The
downsides of this method are that it's quite complex, having a couple of extra steps (disassembly,
reassembly to IL) before the actual analysis phase, it's dependent on the disassembler and the
reassembler needs upkeep in order to maintain support for all different processor architectures.

\section{Linux Kernel}

Blah.

\subsection{Linux Kernel Modules}

Blah.
