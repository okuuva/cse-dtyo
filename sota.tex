% State of the Art Chapter

\section{Software Composition Analysis}

As the third party software and OSS specifically has become a critical part of practically every
bigger software project, commercial and proprietary products included \cite{blah}, it has become
increasingly difficult to keep track of what third party components are included in a given
project. This is quite problematic, as the OSS components included can contain security flaws that
can cause serious harm for the companies and their customers in the form of data loss, leaking
intellectual property and so forth. Luckily there are automatic tools available that can keep
track of third party components included in the project and notify the developers when new
vulnerabitilies are reported in the used components and when there's updates available
\cite{pittenger2016know}.

Software composition analysis has steadily raised its profile over the years \cite{blah}. While it
still hasn't quite reached a mainstream status yet, it has been made available for the masses by
actors such as Docker with the introduction of Docker Security Scan in Docker Hub and Docker Cloud
hosting services \cite{dockerscan}.

Here be some academics.

There are multiple commercial SCA solutions available from providers such as Black Duck Software,
\cite{blackduckhub}, Veracode \cite{veracodesca}, WhiteHat security \cite{whitehatsentinel},
Whitesource \cite{whitesourceosi} and Synopsys \cite{synopsysprotecode}. The main purpose of these
products is to report license information and known vulnerabilities of the identified third party
libraries from the system under test. There are multiple different ways to identify the third party
components. One common way is to statically analyse build files from the source code repository and
deduce what third party libraries are included when the project is compiled. Another way is to try
to figure out what third party library the analyzed code belongs to.

\subsection{Static Binary Analysis}

Static binary analysis is conducted by analyzing the compiled binary without actually executing it.
Working with the binary has some upsides compared to source code: by analyzing the binary we are
analyzing the very same thing the end users are using, thus eliminating the possibility of some
false positives which could result in source code analysis when some or all third party component
dependencies are included using dynamic links by the compiler. There is also the obvious benefit of
not needing to have an access to the source code, which is usually hard or impossible with
proprietary software. However, like Song et al. point out \cite{song2008bitblaze}, there are some
serious down sides to it as well, like additional complexity compared to source code analysis.

\section{Linux Kernel}

Blah.

\subsection{Linux Kernel Modules}

Blah.

