% State of the Art Chapter

\section{Software Composition Analysis}

As the third party software and OSS specifically has become a critical part of practically every
bigger software project, commercial and proprietary products included \cite{blah}, it has become
increasingly difficult to keep track of what third party components are included in a given
project. This is quite problematic, as the OSS components included can contain security flaws that
can cause serious harm for the companies and their customers in the form of data loss, leaking
intellectual property and so forth. Luckily there are automatic tools available that can keep track
of third party components included in the project and notify the developers when new
vulnerabitilies are reported in the used components and when there's updates available
\cite{pittenger2016know}.

Software composition analysis has steadily raised its profile over the years \cite{blah}. While it
still hasn't quite reached a mainstream status yet, it has been made available for the masses by
actors such as Docker with the introduction of Docker Security Scan in Docker Hub and Docker Cloud
hosting services \cite{dockerscan}.

There are multiple commercial SCA solutions available from providers such as Black Duck Software,
\cite{blackduckhub}, Veracode \cite{veracodesca}, WhiteHat security \cite{whitehatsentinel},
Whitesource \cite{whitesourceosi} and Synopsys \cite{synopsysprotecode}. The main purpose of these
products is to report license information and known vulnerabilities of the identified third party
libraries from the system under test. There are multiple different ways to identify the third party
components. One common way is to statically analyse build files from the source code repository and
deduce what third party libraries are included when the project is compiled. Another way is to try
to figure out what third party library the analyzed code belongs to.

\subsection{Static Binary Analysis}

Static binary analysis is conducted by analyzing the compiled binary without actually executing it.
Working with the binary has some upsides compared to source code: by analyzing the binary we are
analyzing the very same thing the end users are using, thus eliminating the possibility of some
false positives which could result in source code analysis when some or all third party component
dependencies are included using dynamic links by the compiler. There is also the obvious benefit of
not needing to have an access to the source code, which is usually hard or impossible with
proprietary software. However, like Song et al. point out \cite{song2008bitblaze}, there are some
serious down sides to it as well, like additional complexity compared to source code analysis and
making the analysis architecture independent, i.e. supporting binaries compiled for different
processor architectures such as ARM, x86 and MIPS.

There are several different methods that can be utilised when conducting SBA. One quite common
approach is using some intermediate language by first running the binary through a disassembler,
then changing the instructions to the IL and finally running the analysis on the resulting pseudo
assembly code \cite{song2008bitblaze, brumley2011bap}. This approach has the advantage of being
more or less architecture independent, given that the tool that converts assembly instructions to
the IL can handle the varying assembly languages of different processor architectures. The
downsides of this method are that it's quite complex, having a couple of extra steps (disassembly,
reassembly to IL) before the actual analysis phase, it's dependent on the disassembler and the
reassembler needs upkeep in order to maintain support for all different processor architectures.

Another common approach in SBA according to Moser, Schmidt et al. \cite{moser2007limits,
schmidt2009static} is signature matching which is mostly used in virus scanners. In this approach,
a signature is generated from the compontents that need to be identified and stored into a
database. This is called the teaching phase. Then in the scanning phase, signatures are being
extracted from the scanned binary and compared against the signatures in the database. If the
extracted signature matches a signature in the database, a match is found and the component can be
said to be included in the scanned binary with some level of certainty. The signature can be
anything depending on the implementation as long as it's unique for each component. A few common
signature types are bytecode signatures, where certain bytecode sequences act as unique signature
and string matching, where static strings residing on the read only section of the binary are used.

The upside of signature based matching is that signatures can be architecture independent to some
extent, especially when using string matching, but on the other hand it requires every component
being separately taught as it can't detect new components without a having a signature of it in the
database. While this is a huge shortcoming of this approach on virus scanners, it's not nearly as
critical in the domain of SCA where missing components can be added to the database when the
shortcomings are detected and the component authors aren't usually deliberately trying to obfuscate
their work unlike the malware producers.

\section{Linux Kernel}

Blah.

\subsection{Linux Kernel Modules}

Blah.
